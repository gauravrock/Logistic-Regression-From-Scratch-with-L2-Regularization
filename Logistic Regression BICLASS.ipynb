{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Axis India Machine Learning Research Lab</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Supervised Machine Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression From Scratch for Biclass Data-set with L2 Regularization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b>It is Dicriminative Learning Algorithm.</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><center>Discriminative Algorithm are those Algorithm for which Inorder to perform Classification i.e to calculate P(class =+ve|X),<br/>we don't need to calculate <b>P(X|class=+ve/-ve)</center></b></p>\n",
    "\n",
    "<b>Discriminent Function : </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{P(class=+ve|X)}{P(class=-ve|X)}=Log_{e}(\\frac{P(class=+ve|X)}{P(class=-ve|X)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{+ve}=\\sum_{-ve}=\\sum_{pooled}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{P(class=+ve|X)}{P(class=-ve|X)}=mx+c\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "Log_{e}(\\frac{P(class=+ve|X)}{P(class=-ve|X)})=\n",
    "\\theta_{1} x+\\theta_{0}\n",
    "$$\n",
    "For Bi-Class\n",
    "\n",
    "$$\n",
    "Log_{e}(\\frac{P(class=+ve|X)}{P(class=-ve|X)})=\n",
    "\\theta_{0} +x_{1}\\theta_{1}+x_{2}\\theta_{2}+x_{3}\\theta_{3}....\n",
    "$$\n",
    "\n",
    "$$\n",
    "    Log_{e}(\\frac{P(class=+ve|X)}{P(class=-ve|X)})=\\theta_{0}+\\theta^{T}X    \\hspace{3cm}       .....eq(i)\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\theta^{T}=\\left[\\theta_{1} \\hspace{2mm}  \\theta_{2}\\hspace{2mm} \\theta_{3} \\hspace{2mm} \\theta_{4} \\hspace{2mm} \\theta_{5}\\right......] \n",
    "$$\n",
    "\n",
    "$$\n",
    "X=\\left[ \\begin{array}{l}{x_{1}} \\\\ {x_{2}} \\\\ {x_{3}} \\\\ {x_{4}} \\\\ {x_{5} \\\\ .\\\\.\\\\.\\\\.}\\end{array}\\right] \\hspace{1.2cm}....Feature's \\ Vector\n",
    "$$\n",
    "\n",
    "Take Log of right hand side to left on eq (i):\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{P(class=+ve|X)}{P(class=-ve|X)}=e^{\\left(\\theta_{0}+\\theta^{T} x\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{P(class=-ve|X)}{P(class=+ve|X)}=e^{-\\left(\\theta_{0}+\\theta^{T} x\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1-P(class=+ve|X)}{P(class=+ve|X)}=e^{-\\left(\\theta_{0}+\\theta^{T} x\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{P(class=+ve|X)}-1=e^{-\\left(\\theta_{0}+\\theta^{T} x\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{P(class=+ve|X)}=1+e^{-\\left(\\theta_{0}+\\theta^{T} x\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "    \\hspace{4cm}    {P(class=+ve|X)}=\\frac{1}{1+e^{-\\left(\\theta_{0}+\\theta^{T} x\\right)}} \\hspace{3cm}....eq(ii)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<b> $$ H(\\theta_{0},\\theta)=\\frac{1}{1+e^{-\\left(\\theta_{0}+\\theta^{T} x\\right)}} $$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic Function or Sigmoid Function:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x)= \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "<h4><center>It's Value Always remain between 0 and 1.</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from <b>eq(ii)</b>,It is clear that\n",
    "\n",
    "$$\n",
    "\\hspace{4cm}    {P(class=-ve|X)}=1-\\frac{1}{1+e^{-\\left(\\theta_{0}+\\theta^{T} x\\right)}} \\hspace{3cm}....eq(iii)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>$$ 1-H(\\theta_{0},\\theta)=1-\\frac{1}{1+e^{-\\left(\\theta_{0}+\\theta^{T} x\\right)}} $$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for any single example ,having class =C ,we define Posterior Probability as :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(class=C|X)=[\\frac{1}{1+e^{-(\\theta_{0}+\\theta^{T}X)}}^{C}] \\dot[1-\\frac{1}{1+e^{-(\\theta_{0}+\\theta^{T}X)}}]^{1-C}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or $$ P(class=C|X)=[{H(\\theta_{0},\\theta)}]^{C} \\dot[1-{H(\\theta_{0},\\theta)}]^{1-C}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for any $ i^{th} $ example from training data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    X^{i}=\\left[ \\begin{array}{l}{x^{i}_{1}} \\\\ {x^{i}_{2}} \\\\ {x^{i}_{3}} \\\\ {x^{i}_{4}} \\\\ {x^{i}_{5} \\\\ .\\\\.\\\\.\\\\.}\\end{array}\\right] \\hspace{8mm} features\\ of\\ i^{th} \\ row\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> $$ H_{i}(\\theta_{0},\\theta_{1})=\\frac{1}{1+e^{-\\left(\\theta_{0}+\\theta^{T} x\n",
    "^{i}\\right)}}                  ..................Eq(iv) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or $$ P(class=C^{i}|X^{i})=[{H_{i}(\\theta_{0},\\theta_{1})}]^{C^{i}} \\dot[1-{H_{i}(\\theta_{0},\\theta_{1})}]^{1-C^{i}} \\hspace{5mm} PDF \\ for \\ single \\ i^{th}  \\ example$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Construct Likelihood  function of Posterior PDF:</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>We will assume that each row of training data is Independent and Identity Distributed and we will construct Likelihood Function</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Assume our training data has 7500 rows of training data and 5 columns(Except Labels Column)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(X=X^{1} \\cap X=X^{2} \\cap X=X^{3} \\cap X=X^{3} .........  \\cap X=X^{7500}) = Likelihood \\ Function$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= \\ [{H_{i}(\\theta_{0},\\theta)}]^{C^{i}} \\dot[1-{H_{i}(\\theta_{0},\\theta)}]^{1-C^{i}}........[{H_{7500}(\\theta_{0},\\theta)}]^{C^{7500}} \\dot[1-{H_{7500}(\\theta_{0},\\theta)}]^{1-C^{7500}}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\prod_{i=1}^{7500}[{H_{i}(\\theta_{0},\\theta)}]^{C^{i}} \\dot[1-{H_{i}(\\theta_{0},\\theta)}]^{1-C^{i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\theta_{0},\\theta)=\\prod_{i=1}^{m}[{H_{i}(\\theta_{0},\\theta)}]^{C^{i}} \\dot[1-{H_{i}(\\theta_{0},\\theta)}]^{1-C^{i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Log_{e}(L(\\theta_{0},\\theta))=\\frac{1}{m}\\Sigma_{i=1}^{m}Log_{e}[{H_{i}(\\theta_{0},\\theta)}]^{C^{i}} \\dot[1-{H_{i}(\\theta_{0},\\theta)}]^{1-C^{i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -Log_{e}(L(\\theta_{0},\\theta))=-\\frac{1}{m}\\Sigma_{i=1}^{m}[({C^{i}})Log_{e}{H_{i}(\\theta_{0},\\theta)}] +[({1-C^{i}})Log_{e}(1-{H_{i}(\\theta_{0},\\theta)})] \\hspace{5mm} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>After Adding L2 Regularization Term,Cost Function will be :</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -Log_{e}(L(\\theta_{0},\\theta))=-[\\frac{1}{m}\\Sigma_{i=1}^{m}[({C^{i}})Log_{e}{H_{i}(\\theta_{0},\\theta)}] +[({1-C^{i}})Log_{e}(1-{H_{i}(\\theta_{0},\\theta)})]] + \\frac{\\lambda}{2m}\\Sigma_{j=1}^{n}\\theta_{j}^{2} \\hspace{5mm} ....Eq(v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Using \\ Gradient \\ Descent \\ to \\ minimize\\ -Log_{e}(L(\\theta_{0},\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For Gradient Descent:</b><br/>\n",
    "    \n",
    "    \n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while(True):\n",
    "$$\\theta^{new}=\\theta^{old} - alpha * [ \\frac{\\partial}{\\partial\\theta}[Log_{e}(L(\\theta_{0},\\theta))]] $$\n",
    "$$if \\ abs((L(\\theta_{0}^{old},\\theta^{old})) -(L(\\theta_{0}^{new},\\theta^{new})) ) < \\ epsilion: \\\\break$$\n",
    "\n",
    "$$ \\theta^{old}=\\theta^{new} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1. By \\ solving \\ \\frac{\\partial}{\\partial\\theta_{0}}[Log_{e}(L(\\theta_{0},\\theta))] ,we\\ get $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{0} :=\\theta_{0}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{0}^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$2. By \\ solving \\ \\frac{\\partial}{\\partial\\theta}[Log_{e}(L(\\theta_{0},\\theta))] ,we\\ get $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta :=\\theta-\\alpha\\left[\\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x^{(i)}-\\frac{\\lambda}{m} \\theta\\right]\n",
    "$$\n",
    "\n",
    "$$ \\theta=[ \\theta_{1},\\theta_{2},\\theta_{3},...] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}{\\text {Hence ,In Gradient descent }} \\\\ {\\text { Repeat }\\{ } \\\\ {\\Rightarrow \\quad \\theta_{0} :=\\theta_{0}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}-\\frac{\\lambda}{m} \\theta_{j} ]} \\\\ {\\Rightarrow \\quad\\theta :=\\theta-\\alpha\\left[\\frac{1}{m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x^{(i)}-\\frac{\\lambda}{m} \\theta\\right]}\\\\ \\hspace{20mm} \\theta=[ \\theta_{1},\\theta_{2},\\theta_{3},...]\\\\ {\\}}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Coding For Logistic Regression:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Importing Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "###\n",
    "warnings.filterwarnings(\"ignore\");\n",
    "\n",
    "###\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Importing Dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Banknote authentication Data Set .</h6>\n",
    "<h6>Abstract: </h6><p>Data were extracted from images that were taken for the evaluation of an authentication procedure for banknotes.</p>\n",
    "<h6>Data Set Information:</h6>\n",
    "\n",
    "<p>Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.</p>\n",
    "\n",
    "<h6>Attribute Information:</h6>\n",
    "\n",
    "<p>1. variance of Wavelet Transformed image (continuous) </p>\n",
    "<p>2. skewness of Wavelet Transformed image (continuous) </p>\n",
    "<p>3. curtosis of Wavelet Transformed image (continuous) </p>\n",
    "<p>4. entropy of image (continuous) </p>\n",
    "<p>5. class (integer)</p>\n",
    "\n",
    "<p>To View or Download the Dataset ,<a href=\"https://archive.ics.uci.edu/ml/datasets/banknote+authentication\">Click here</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv(\"biclass_lr.csv\",names=[\"variance\", \"skewness\", \"curtosis\", \"entropy\",\"Output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Shuffling of Data .</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.utils\n",
    "Data = sklearn.utils.shuffle(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Analytics of Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    762\n",
       "1    610\n",
       "Name: Output, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Class_column=\"Output\"\n",
    "\n",
    "#Counting Total numbers of Record for each Class\n",
    "Data[Class_column].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Dividing Dataset into Training Data And Testing Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data_set,split_ratio):\n",
    "    return (data_set.iloc[0:(int(len(data_set)*split_ratio)),:],data_set.iloc[(int(len(data_set)*split_ratio)):,:]) \n",
    "\n",
    "#Call to function train_test_split with dataset_name and ratio on which dataset will into training and testing data\n",
    "training_data,testing_data=train_test_split(Data,0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Pre-processing Training Data for Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def filtered_training_data(training_data_part,class_label):\n",
    "    \n",
    "    #Take Class Column out from Training Data and Store it to Another variable.\n",
    "    #Converting class_Column into array\n",
    "    #Dropping class Column from Training Data\n",
    "    \n",
    "    return(np.array(training_data_part[class_label]),training_data_part.drop(labels=class_label,axis=1))\n",
    "\n",
    "#Call to Function\n",
    "class_of_training_data,filtered_training_data=filtered_training_data(training_data,Class_column)\n",
    "\n",
    "\n",
    "###\n",
    "def normalize(data_to_noramlize):\n",
    "    \n",
    "    #Converting Training Data from type Dataframe to Array for Matrix Operation\n",
    "    pure_training_data=np.array(data_to_noramlize)\n",
    "    \n",
    "    #Dividing Each Column of Training Data with respective Column Maximium Value to Normalize the Dataset\n",
    "    return(pure_training_data/np.max(pure_training_data,axis=0))\n",
    "\n",
    "#Call to Function\n",
    "normalized_Data=normalize(filtered_training_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7. Preparation of Initial Parameter,Sigmoid function and calculation of derivative for gradient Descent </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_parameter(normalized_Data_part):\n",
    "    \n",
    "    #Generating Initila HyperParameter Values\n",
    "    return(np.random.randn(1,1),np.random.randn(normalized_Data_part.shape[1],1))\n",
    "\n",
    "\n",
    "def sigmoid(normalized_Data_part,theta0_initial_val,theta_initial_val):\n",
    "    \n",
    "    #Multiplying Training Data with theta_initial and adding theta0_initial in all terms after mat. mul. operation.\n",
    "    power=theta0_initial_val+np.dot(normalized_Data_part,theta_initial_val)\n",
    "    \n",
    "    #Calculating eq (iii) for all training dataset\n",
    "    return(1/(1+ np.exp(-power)))\n",
    "\n",
    "\n",
    "def deri_wrt_theta0(temp,normalized_Data_part):\n",
    "    \n",
    "    #Derivative of Cost Function w.r.t to theta0 to Upgrade theta0 during Minimization\n",
    "    return((np.sum(temp))/normalized_Data_part.shape[0])\n",
    "\n",
    "\n",
    "def deri_wrt_theta(temp,normalized_Data_part,lam,theta_initial):\n",
    "    \n",
    "    \n",
    "    ##Derivative of Cost Function w.r.t to thetas(except theta0) to Upgrade thetas during Minimization - L2 Regularization \n",
    "    #term on upgrading thetas\n",
    "    return(((np.matmul((normalized_Data_part.T),temp))/normalized_Data_part.shape[0]) -\n",
    "           ((lam/normalized_Data_part.shape[0])*theta_initial))\n",
    "\n",
    "\n",
    "def cost_function(class_of_training_data,H_theta0_theta_A,theta_A,normalized_Data_part,lam):\n",
    "    a=np.matmul((class_of_training_data.T),np.log(H_theta0_theta_A))\n",
    "    b=np.matmul(((1-class_of_training_data).T),np.log(1-H_theta0_theta_A))\n",
    "    c=(a+b)/normalized_Data_part.shape[0]\n",
    "    d=(lam*np.sum(np.square(theta_A)))/2*normalized_Data_part.shape[0]\n",
    "    return(-c+d)\n",
    "    \n",
    "##just For Removing Ambiguity ,size is change from (n,) to (n,1)\n",
    "##No difference in overall shape and Data \n",
    "class_of_training_data=np.reshape(class_of_training_data,(training_data.shape[0],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8. Gradient Descent to minimize Cost Function </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(alpha,epsilon,normalized_Data,class_of_training_data,lam):\n",
    "    #parameter's\n",
    "    #Alpha(Learning Rate)\n",
    "    #Epsilon : to stop gradient descent at proper minimized state\n",
    "    #lam(lambda) : For L2 Regularisation\n",
    "    \n",
    "    \n",
    "    #Call to Function\n",
    "    theta0_initial,theta_initial=initial_parameter(normalized_Data)\n",
    "    \n",
    "    i=0\n",
    "    iterations=[]\n",
    "    neg_log_like_loss = []\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        #Call to Function\n",
    "        #Equation iv at theta0_initial ,theta_initial\n",
    "        H_theta0_theta_i=sigmoid(normalized_Data,theta0_initial,theta_initial)\n",
    "        \n",
    "    \n",
    "        #Calculating Difference between calculated H_theta0_theta_i and given class_of_training_data\n",
    "        Tem =H_theta0_theta_i-class_of_training_data\n",
    "        \n",
    "        #Call to Functions\n",
    "        Derivative_theta0 =deri_wrt_theta0(Tem,normalized_Data)\n",
    "        Derivative_thetas =deri_wrt_theta(Tem,normalized_Data,lam,theta_initial)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Equation No v at theta0_initial ,theta_initial with L2 Regularization\n",
    "        neg_log_like_initial= cost_function(class_of_training_data,H_theta0_theta_i,theta_initial,normalized_Data,lam)\n",
    "        \n",
    "        \n",
    "        #Updating the hyperparameters value\n",
    "        theta0_final = theta0_initial - alpha*Derivative_theta0\n",
    "        theta_final = theta_initial - alpha*Derivative_thetas\n",
    "        \n",
    "    \n",
    "        ##Equation iv at theta0_final,theta_final\n",
    "        H_theta0_theta_f = sigmoid(normalized_Data,theta0_final,theta_final)\n",
    "    \n",
    "        #Equation No v at theta0_final,theta_final with L2 Regularization\n",
    "        neg_log_like_final= cost_function(class_of_training_data,H_theta0_theta_f,theta_final,normalized_Data,lam)\n",
    "        \n",
    "        if abs(neg_log_like_initial - neg_log_like_final) < epsilon:\n",
    "            return(theta0_final,theta_final,iterations,neg_log_like_loss)\n",
    "            break\n",
    "            \n",
    "        theta0_initial = theta0_final\n",
    "        theta_initial = theta_final\n",
    "    \n",
    "        i += 1\n",
    "    \n",
    "        iterations.append(i)\n",
    "        neg_log_like_loss.append(neg_log_like_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call to gradient_Descent Function\n",
    "theta0_final,theta_final,iterations,neg_log_like_loss=gradient_descent(0.01,0.0000005\n",
    "                                                                       ,normalized_Data,class_of_training_data,0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Minimization Curve of Cost Function in Logistic Regression')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(iterations,neg_log_like_loss,data=\"Logistic Regression\")\n",
    "plt.title('Minimization Curve of Cost Function in Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>9.Testing of Model WIth Testing Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing of testing data\n",
    "\n",
    "def testing(testing_data,Class_column):\n",
    "    \n",
    "    #Take Class Column out from Testing Data and Store it to Another variable.\n",
    "    #Converting class_Column into List(for Easy Implementation )\n",
    "    #Dropping class Column from Testing Data\n",
    "    #Normalizing the testing data\n",
    "    \n",
    "    class_of_testing_data=list(testing_data[Class_column])\n",
    "    filtered_testing_data=testing_data.drop(labels=Class_column,axis=1)\n",
    "    normalized_testing_Data=normalize(filtered_testing_data)\n",
    "    \n",
    "    return(class_of_testing_data,normalized_testing_Data)\n",
    "\n",
    "#Call to Function\n",
    "class_of_testing_data,normalized_testing_Data=testing(testing_data,Class_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>10. Checking Accuracy and other Efficiency Measuring Parameter of Model Performance.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Of Model on Testing Data :  0.9737609329446064\n",
      "Confusion Matrix :\n",
      "\n",
      " [[181   5]\n",
      " [  4 153]]\n",
      "\n",
      "Completer Classification Report on Testing Data :\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.98       186\n",
      "          1       0.97      0.97      0.97       157\n",
      "\n",
      "avg / total       0.97      0.97      0.97       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prediction of class \n",
    "def accuracy(normalized_testing_Data,theta0_final,theta_final):\n",
    "    output_list=[]\n",
    "    \n",
    "    #calculating sigmoid function value for each data row in testing data\n",
    "    sigmoid_function_value=sigmoid(normalized_testing_Data,theta0_final,theta_final)\n",
    "     \n",
    "    for i in sigmoid_function_value:\n",
    "        if i>=0.50:\n",
    "            output_list.append(1)\n",
    "        else:\n",
    "            output_list.append(0)\n",
    "    print(\"Accuracy Of Model on Testing Data : \",accuracy_score(class_of_testing_data,output_list))\n",
    "    print(\"Confusion Matrix :\\n\\n\",confusion_matrix(class_of_testing_data,output_list))\n",
    "   \n",
    "    print(\"\\nCompleter Classification Report on Testing Data :\\n\\n\",classification_report(class_of_testing_data,output_list))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#Call to Accuracy function to check Model Accuracy and others    \n",
    "accuracy(normalized_testing_Data,theta0_final,theta_final)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Hence ,Our Model is Able to correclty Predict Class for unseen Data about approx. 96% times.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
